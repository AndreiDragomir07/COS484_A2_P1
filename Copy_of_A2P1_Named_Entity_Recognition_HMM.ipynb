{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreiDragomir07/COS484_A2_P1/blob/main/Copy_of_A2P1_Named_Entity_Recognition_HMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1sXm2wt519o"
      },
      "source": [
        "# Programming Problem 1: HMM for NER\n",
        "Welcome to the programming portion of the assignment! Each assignment throughout the semester will have a written portion and a programming portion. We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
        "\n",
        "### Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to add and delete arguments in function signatures, but be careful that you might need to change them in function calls which are already present in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEyk2ch1GYi0"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "In this section we will write code to load data and build the dataset for Named Entity Recognition.\n",
        "\n",
        "You may inspect the data first before writing the data preprocessing code by looking at the data file: https://princeton-nlp.github.io/cos484/assignments/a2/eng.train. Hints on processing the data:\n",
        "- You may ignore the lines with \"DOCSTART\"\n",
        "- Examples of NER tags include \"O\", \"ORG\", \"MISC\", and it's always in the same position in each line of the data.\n",
        "- To process numbers more easily, you can replace all digits with 0's (to avoid out-of-vocab words)\n",
        "\n",
        "You should end up with a list of sentences, where each sentence is represented with a list of words and tags.\n",
        "\n",
        "Additional, you will want to support the following functions for later:\n",
        "- Map words and tags to ids (integers)\n",
        "- Handle unknown words in mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lNS57L87IQdT"
      },
      "outputs": [],
      "source": [
        "class PreprocessData:\n",
        "    \"\"\"\n",
        "    Preprocess the data: build a list of sentences, where each sentence is a list of (word, tag) tuples.\n",
        "    Also builds word-to-id and tag-to-id mappings.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.sentences = []\n",
        "        current_sentence = []\n",
        "        # set() creates an empty set, which is an unordered collection of unique elements in Python.\n",
        "        word_set = set()\n",
        "        tag_set = set()\n",
        "        for line in data:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if current_sentence:\n",
        "                    self.sentences.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            word, tag = parts[0], parts[3]\n",
        "            if word == \"-DOCSTART-\":\n",
        "                continue\n",
        "            word = re.sub(r'\\d', '0', word)\n",
        "            current_sentence.append((word, tag))\n",
        "            word_set.add(word)\n",
        "            tag_set.add(tag)\n",
        "        if current_sentence:\n",
        "            self.sentences.append(current_sentence)\n",
        "        # Build word-to-id and tag-to-id dictionaries. Reserve 0 for <UNK> for words.\n",
        "        self.word2id = {word: idx+1 for idx, word in enumerate(sorted(word_set))}\n",
        "        self.word2id[\"<UNK>\"] = 0\n",
        "        self.tag2id = {tag: idx+1 for idx, tag in enumerate(sorted(tag_set))}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "train_data_url = \"https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\"\n",
        "response = requests.get(train_data_url)\n",
        "response.raise_for_status() # Raise an exception for HTTP errors\n",
        "train_data_lines = response.text.splitlines()\n",
        "\n",
        "print(f\"Successfully downloaded {len(train_data_lines)} lines of training data.\")\n",
        "print(\"First 5 lines:\")\n",
        "for i in range(min(5, len(train_data_lines))):\n",
        "    print(train_data_lines[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLna66tj5TX2",
        "outputId": "baff7f30-5d99-4153-eaff-0a9ed31ffedb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded 219553 lines of training data.\n",
            "First 5 lines:\n",
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_data = PreprocessData(train_data_lines)\n",
        "\n",
        "print(f\"Number of sentences in training data: {len(processed_train_data.sentences)}\")\n",
        "print(f\"Number of unique words in training data: {len(processed_train_data.word2id)}\")\n",
        "print(f\"Number of unique tags in training data: {len(processed_train_data.tag2id)}\")\n",
        "print(\"First sentence (word, tag) tuples:\")\n",
        "print(processed_train_data.sentences[0])\n",
        "print(\"First 5 word to ID mappings:\")\n",
        "for i, (word, idx) in enumerate(processed_train_data.word2id.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"  {word}: {idx}\")\n",
        "print(\"First 5 tag to ID mappings:\")\n",
        "for i, (tag, idx) in enumerate(processed_train_data.tag2id.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"  {tag}: {idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNUzywTD5UXY",
        "outputId": "65dee0d2-0a57-4a4d-f837-9a4831e1cd3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in training data: 14041\n",
            "Number of unique words in training data: 20101\n",
            "Number of unique tags in training data: 5\n",
            "First sentence (word, tag) tuples:\n",
            "[('EU', 'ORG'), ('rejects', 'O'), ('German', 'MISC'), ('call', 'O'), ('to', 'O'), ('boycott', 'O'), ('British', 'MISC'), ('lamb', 'O'), ('.', 'O')]\n",
            "First 5 word to ID mappings:\n",
            "  !: 1\n",
            "  \": 2\n",
            "  $: 3\n",
            "  %: 4\n",
            "  &: 5\n",
            "First 5 tag to ID mappings:\n",
            "  LOC: 1\n",
            "  MISC: 2\n",
            "  O: 3\n",
            "  ORG: 4\n",
            "  PER: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgikvX-qKTL7"
      },
      "source": [
        "### Hidden Markov Model\n",
        "In this section, we will implement a bigram hidden markov model (HMM) that could perform two types of decoding: greedy decoding and viterbi decoding.\n",
        "\n",
        "Specifically, you should include the following functionalities:\n",
        "- Initialize the HMM given the word and tag mappings.\n",
        "- Train the HMM with a given corpus\n",
        "- Greedy decoding: given a single sentence, output its tags with greedy algorithm\n",
        "- Viterbi decoding: given a single sentence, output its tags using Viterbi\n",
        "\n",
        "You may refer to the lecture notes for more details on the HMM and the decoding algorithms."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class HMM_Train:\n",
        "    \"\"\"\n",
        "    Trains an HMM by counting tag-to-tag transitions and tag-to-word emissions.\n",
        "    Sentence start is treated as a void tag at position 0 (transition from void to first tag).\n",
        "    - transition_counts: (num_tags+1, num_tags+1). Row 0 = counts from void (start of sentence) to each tag.\n",
        "    - emission_counts: (num_tags+1, num_words); emission_counts[tag_idx][word_id] = count.\n",
        "    \"\"\"\n",
        "    def __init__(self, sentences, word2id, tag2id):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sentences: list of sentences, each sentence is a list of (word, tag) tuples\n",
        "            word2id: dict mapping word -> int id\n",
        "            tag2id: dict mapping tag -> int id. Assumes tag IDs start from 1, and 0 is not occupied.\n",
        "        \"\"\"\n",
        "        self.sentences = sentences\n",
        "        self.word2id = word2id\n",
        "        self.tag2id = tag2id\n",
        "        num_tags = len(tag2id)\n",
        "        num_words = len(word2id)\n",
        "\n",
        "        # Transition counts: (num_tags + 1, num_tags + 1)\n",
        "        # Row 0 = from void (start of sentence)\n",
        "        # Columns 0 = to void (will remain 0), 1..num_tags = to tag id 1..num_tags\n",
        "        self.transition_counts = np.zeros((num_tags + 1, num_tags + 1), dtype=np.float64)\n",
        "\n",
        "        # Emission counts: (num_tags + 1, num_words)\n",
        "        # Row 0 = for void (will remain 0), rows 1..num_tags = for tag id 1..num_tags\n",
        "        self.emission_counts = np.zeros((num_tags + 1, num_words), dtype=np.float64)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            prev_tag_id = 0  # 0 = void (start of sentence)\n",
        "            for word, tag in sentence:\n",
        "                word_id = word2id.get(word, 0) # 0 for <UNK>\n",
        "                tag_id = tag2id[tag] # tag_id is 1-indexed\n",
        "\n",
        "                # Use 1-indexed tag_id directly for column in transition_counts\n",
        "                self.transition_counts[prev_tag_id, tag_id] += 1\n",
        "                # Use 1-indexed tag_id directly for row in emission_counts\n",
        "                self.emission_counts[tag_id, word_id] += 1\n",
        "                prev_tag_id = tag_id\n",
        "\n",
        "    def counts_to_probabilities(self, counts_matrix, k=0.0):\n",
        "        \"\"\"\n",
        "        Converts a count matrix into a probability matrix using add-k smoothing.\n",
        "\n",
        "        Args:\n",
        "            counts_matrix (np.ndarray): The matrix of counts.\n",
        "            k (float): The smoothing parameter (add-k smoothing). Default is 0.0 (no smoothing).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The probability matrix.\n",
        "        \"\"\"\n",
        "        if k == 0.0:\n",
        "            # Avoid division by zero for rows that sum to 0 if no smoothing\n",
        "            row_sums = np.sum(counts_matrix, axis=1, keepdims=True)\n",
        "            # Replace 0 sums with 1 to prevent division by zero, resulting in 0 probabilities for that row\n",
        "            row_sums[row_sums == 0] = 1\n",
        "            probabilities = counts_matrix / row_sums\n",
        "        else:\n",
        "            num_columns = counts_matrix.shape[1]\n",
        "            smoothed_counts = counts_matrix + k\n",
        "            denominator = np.sum(counts_matrix, axis=1, keepdims=True) + k * num_columns\n",
        "            probabilities = smoothed_counts / denominator\n",
        "        return probabilities\n"
      ],
      "metadata": {
        "id": "1T2A4VWNY-WC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aee37d74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa96fa01-7f2d-4318-e617-60f718885ad8"
      },
      "source": [
        "hmm_trainer = HMM_Train(\n",
        "    processed_train_data.sentences,\n",
        "    processed_train_data.word2id,\n",
        "    processed_train_data.tag2id\n",
        ")\n",
        "\n",
        "# Calculate transition probabilities (with smoothing)\n",
        "transition_probabilities = hmm_trainer.counts_to_probabilities(\n",
        "hmm_trainer.transition_counts, k=0.1\n",
        ")\n",
        "\n",
        "# Explicitly set the 'to void' probabilities (column 0) to zero, then re-normalize.\n",
        "# This assumes that the 'void' state (tag_id = 0) is never a destination.\n",
        "transition_probabilities[:, 0] = 0.0 # Set all probabilities of transitioning TO void to 0\n",
        "\n",
        "# Re-normalize rows to ensure probabilities sum to 1 after zeroing.\n",
        "# This is crucial for valid probabilities for subsequent calculations.\n",
        "row_sums_after_zeroing = np.sum(transition_probabilities, axis=1, keepdims=True)\n",
        "# Avoid division by zero for rows that might still sum to 0 after zeroing\n",
        "row_sums_after_zeroing[row_sums_after_zeroing == 0] = 1\n",
        "transition_probabilities = transition_probabilities / row_sums_after_zeroing\n",
        "\n",
        "# Calculate emission probabilities (add-10 smoothing)\n",
        "emission_probabilities = hmm_trainer.counts_to_probabilities(\n",
        "hmm_trainer.emission_counts, k=1.0\n",
        ")\n",
        "\n",
        "# Modify the following print statement to print out the transition count matrix:\n",
        "with np.printoptions(suppress=True):\n",
        "    print(\"The transition count matrix is the following: \\n\", hmm_trainer.transition_counts)\n",
        "\n",
        "print(\"Shape of transition probabilities matrix:\", transition_probabilities.shape)\n",
        "print(\"Shape of emission probabilities matrix:\", emission_probabilities.shape)\n",
        "\n",
        "print(\"\\nFull transition probabilities matrix:\")\n",
        "with np.printoptions(suppress=True, threshold=np.inf):\n",
        "    print(transition_probabilities)\n",
        "\n",
        "print(\"\\nFirst 5 rows and 5 columns of emission probabilities:\")\n",
        "print(emission_probabilities[:5, :5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The transition count matrix is the following: \n",
            " [[     0.   1581.    502.   8154.   2455.   1349.]\n",
            " [     0.   1168.     30.   6927.     11.      1.]\n",
            " [     0.     10.   1190.   3152.     39.     61.]\n",
            " [     0.   5533.   2861. 138886.   3792.   5183.]\n",
            " [     0.      2.      9.   6105.   3728.      6.]\n",
            " [     0.      3.      1.   6354.      0.   4528.]]\n",
            "Shape of transition probabilities matrix: (6, 6)\n",
            "Shape of emission probabilities matrix: (6, 20101)\n",
            "\n",
            "Full transition probabilities matrix:\n",
            "[[0.         0.11260193 0.03575829 0.58071431 0.17484599 0.09607948]\n",
            " [0.         0.14354531 0.00369892 0.85125653 0.00136406 0.00013518]\n",
            " [0.         0.00226839 0.26728804 0.70793936 0.00878158 0.01372263]\n",
            " [0.         0.03541059 0.0183104  0.88883975 0.02426859 0.03317067]\n",
            " [0.         0.00021319 0.00092381 0.61977565 0.3784681  0.00061926]\n",
            " [0.         0.00028476 0.00010104 0.58366785 0.00000919 0.41593717]]\n",
            "\n",
            "First 5 rows and 5 columns of emission probabilities:\n",
            "[[4.97487687e-05 4.97487687e-05 4.97487687e-05 4.97487687e-05\n",
            "  4.97487687e-05]\n",
            " [3.52137474e-05 3.52137474e-05 3.52137474e-05 3.52137474e-05\n",
            "  3.52137474e-05]\n",
            " [4.04956670e-05 8.09913339e-05 4.04956670e-05 4.04956670e-05\n",
            "  4.04956670e-05]\n",
            " [5.27206491e-06 1.05441298e-05 1.14878294e-02 1.91375956e-03\n",
            "  1.26529558e-04]\n",
            " [3.31939189e-05 3.31939189e-05 3.31939189e-05 3.31939189e-05\n",
            "  3.31939189e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4b16ce9"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "class HMM:\n",
        "    def __init__(self, word2id, tag2id, transition_probabilities, emission_probabilities):\n",
        "        self.word2id = word2id\n",
        "        self.tag2id = tag2id\n",
        "        self.id2tag = {v: k for k, v in tag2id.items()}\n",
        "        self.transition_probabilities = transition_probabilities\n",
        "        self.emission_probabilities = emission_probabilities\n",
        "        self.num_tags = len(tag2id)\n",
        "\n",
        "    def greedy_decode(self, sentence):\n",
        "        predicted_tags_ids = []\n",
        "        prev_tag_id = 0  # 0 represents the 'void' start state\n",
        "\n",
        "        for word in sentence:\n",
        "            word_id = self.word2id.get(word, 0) # 0 for <UNK>\n",
        "\n",
        "            best_score = -np.inf\n",
        "            best_tag_id = -1\n",
        "\n",
        "            for current_tag_idx in range(1, self.num_tags + 1): # Iterate through 1-indexed tag IDs\n",
        "\n",
        "                # prev_tag_id (0 for void, or 1-indexed tag ID) as row index\n",
        "                # current_tag_idx (1-indexed tag ID) as column index\n",
        "                trans_prob = self.transition_probabilities[prev_tag_id, current_tag_idx]\n",
        "\n",
        "                # current_tag_idx (1-indexed tag ID) as row index\n",
        "                emit_prob = self.emission_probabilities[current_tag_idx, word_id]\n",
        "\n",
        "                score = np.log(trans_prob) + np.log(emit_prob)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_tag_id = current_tag_idx # current_tag_idx is already 1-indexed\n",
        "\n",
        "            predicted_tags_ids.append(best_tag_id)\n",
        "            prev_tag_id = best_tag_id # Update previous tag for next iteration\n",
        "\n",
        "        return [self.id2tag[tag_id] for tag_id in predicted_tags_ids]\n",
        "\n",
        "    def viterbi_decode(self, sentence):\n",
        "        sentence_len = len(sentence)\n",
        "\n",
        "        # Viterbi matrix: V[tag_id][word_idx] stores the max log probability\n",
        "        # Backpointer matrix: B[tag_id][word_idx] stores the previous tag_id\n",
        "        # Using num_tags + 1 for rows to directly map 1-indexed tag_ids.\n",
        "        V = np.full((self.num_tags + 1, sentence_len), -np.inf)\n",
        "        B = np.zeros((self.num_tags + 1, sentence_len), dtype=int)\n",
        "\n",
        "        # --- Initialization (t=0) ---\n",
        "        first_word_id = self.word2id.get(sentence[0], 0)\n",
        "        for current_tag_idx in range(1, self.num_tags + 1):\n",
        "            # From void (state 0) to current tag\n",
        "            trans_prob = self.transition_probabilities[0, current_tag_idx]\n",
        "            emit_prob = self.emission_probabilities[current_tag_idx, first_word_id]\n",
        "            V[current_tag_idx, 0] = np.log(trans_prob) + np.log(emit_prob)\n",
        "            B[current_tag_idx, 0] = 0 # No previous tag for the first word (from void)\n",
        "\n",
        "        # --- Recursion (t=1 to sentence_len - 1) ---\n",
        "        for t in range(1, sentence_len):\n",
        "            word_id = self.word2id.get(sentence[t], 0)\n",
        "            for current_tag_idx in range(1, self.num_tags + 1):\n",
        "                best_prev_score = -np.inf\n",
        "                best_prev_tag_id = -1\n",
        "                for prev_tag_idx in range(1, self.num_tags + 1):\n",
        "                    score = V[prev_tag_idx, t-1] \\\n",
        "                            + np.log(self.transition_probabilities[prev_tag_idx, current_tag_idx]) \\\n",
        "                            + np.log(self.emission_probabilities[current_tag_idx, word_id])\n",
        "\n",
        "                    if score > best_prev_score:\n",
        "                        best_prev_score = score\n",
        "                        best_prev_tag_id = prev_tag_idx\n",
        "                V[current_tag_idx, t] = best_prev_score\n",
        "                B[current_tag_idx, t] = best_prev_tag_id\n",
        "\n",
        "        # --- Termination ---\n",
        "        # Find the path with the highest probability at the last word\n",
        "        last_word_col = sentence_len - 1\n",
        "        best_last_tag_id = np.argmax(V[1:, last_word_col]) + 1 # +1 because V[0,:] is unused for tags\n",
        "        # max_log_prob = V[best_last_tag_id, last_word_col] # Not explicitly needed for path reconstruction\n",
        "\n",
        "        # --- Backtracking ---\n",
        "        predicted_tags_ids = [0] * sentence_len\n",
        "        predicted_tags_ids[last_word_col] = best_last_tag_id\n",
        "\n",
        "        for t in range(sentence_len - 1, 0, -1):\n",
        "            predicted_tags_ids[t-1] = B[predicted_tags_ids[t], t]\n",
        "\n",
        "        # Convert tag IDs to actual tag names\n",
        "        return [self.id2tag[tag_id] for tag_id in predicted_tags_ids]\n",
        "\n",
        "    def evaluate_accuracy(self, test_sentences, decoding_method='viterbi'):\n",
        "        if decoding_method not in ['greedy', 'viterbi']:\n",
        "            raise ValueError(\"decoding_method must be 'greedy' or 'viterbi'\")\n",
        "\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for sentence_data in test_sentences:\n",
        "            words = [item[0] for item in sentence_data]\n",
        "            true_tags = [item[1] for item in sentence_data]\n",
        "\n",
        "            if decoding_method == 'greedy':\n",
        "                predicted_tags = self.greedy_decode(words)\n",
        "            else:\n",
        "                predicted_tags = self.viterbi_decode(words)\n",
        "\n",
        "            for i in range(len(true_tags)):\n",
        "                total_predictions += 1\n",
        "                if predicted_tags[i] == true_tags[i]:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "        return accuracy\n",
        "\n",
        "    def compute_confusion_matrix(self, test_sentences, decoding_method='viterbi'):\n",
        "        if decoding_method not in ['greedy', 'viterbi']:\n",
        "            raise ValueError(\"decoding_method must be 'greedy' or 'viterbi'\")\n",
        "\n",
        "        all_true_tags = []\n",
        "        all_predicted_tags = []\n",
        "\n",
        "        for sentence_data in test_sentences:\n",
        "            words = [item[0] for item in sentence_data]\n",
        "            true_tags = [item[1] for item in sentence_data]\n",
        "\n",
        "            if decoding_method == 'greedy':\n",
        "                predicted_tags = self.greedy_decode(words)\n",
        "            else:\n",
        "                predicted_tags = self.viterbi_decode(words)\n",
        "\n",
        "            all_true_tags.extend(true_tags)\n",
        "            all_predicted_tags.extend(predicted_tags)\n",
        "\n",
        "        # Get all unique tags from tag2id, sorted by their ID for consistent order\n",
        "        labels = sorted(self.tag2id.keys(), key=lambda x: self.tag2id[x])\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(all_true_tags, all_predicted_tags, labels=labels)\n",
        "        return cm, labels\n",
        "\n",
        "    def compute_f1_score(self, test_sentences, decoding_method='viterbi', average='weighted'):\n",
        "        if decoding_method not in ['greedy', 'viterbi']:\n",
        "            raise ValueError(\"decoding_method must be 'greedy' or 'viterbi'\")\n",
        "\n",
        "        all_true_tags = []\n",
        "        all_predicted_tags = []\n",
        "\n",
        "        for sentence_data in test_sentences:\n",
        "            words = [item[0] for item in sentence_data]\n",
        "            true_tags = [item[1] for item in sentence_data]\n",
        "\n",
        "            if decoding_method == 'greedy':\n",
        "                predicted_tags = self.greedy_decode(words)\n",
        "            else:\n",
        "                predicted_tags = self.viterbi_decode(words)\n",
        "\n",
        "            all_true_tags.extend(true_tags)\n",
        "            all_predicted_tags.extend(predicted_tags)\n",
        "\n",
        "        labels = sorted(self.tag2id.keys(), key=lambda x: self.tag2id[x])\n",
        "\n",
        "        # Compute F1 score\n",
        "        f1 = f1_score(all_true_tags, all_predicted_tags, labels=labels, average=average)\n",
        "        return f1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the HMM model with the calculated probabilities and mappings\n",
        "hmm_model = HMM(\n",
        "    processed_train_data.word2id,\n",
        "    processed_train_data.tag2id,\n",
        "    transition_probabilities,\n",
        "    emission_probabilities\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\n--- Evaluating on Training Data ---\")\n",
        "\n",
        "# Evaluate with greedy decoding on full training set\n",
        "greedy_train_accuracy = hmm_model.evaluate_accuracy(processed_train_data.sentences, decoding_method='greedy')\n",
        "print(f\"Greedy decoding accuracy on training data: {greedy_train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate with Viterbi decoding on full training set\n",
        "viterbi_train_accuracy = hmm_model.evaluate_accuracy(processed_train_data.sentences, decoding_method='viterbi')\n",
        "print(f\"Viterbi decoding accuracy on training data: {viterbi_train_accuracy:.4f}\")\n",
        "\n",
        "# Compute Confusion Matrix for greedy decoding on training data\n",
        "cm_greedy, cm_labels = hmm_model.compute_confusion_matrix(processed_train_data.sentences, decoding_method='greedy')\n",
        "print(f\"\\nConfusion Matrix (Greedy, training data):\\n{cm_greedy}\")\n",
        "print(f\"Confusion Matrix Labels: {cm_labels}\")\n",
        "\n",
        "# Compute F1 Score for greedy decoding on training data\n",
        "f1_greedy = hmm_model.compute_f1_score(processed_train_data.sentences, decoding_method='greedy')\n",
        "print(f\"F1 Score (Greedy, training data): {f1_greedy:.4f}\")\n",
        "\n",
        "# Compute Confusion Matrix for Viterbi decoding on training data\n",
        "cm_viterbi, _ = hmm_model.compute_confusion_matrix(processed_train_data.sentences, decoding_method='viterbi')\n",
        "print(f\"\\nConfusion Matrix (Viterbi, training data):\\n{cm_viterbi}\")\n",
        "\n",
        "# Compute F1 Score for Viterbi decoding on training data\n",
        "f1_viterbi = hmm_model.compute_f1_score(processed_train_data.sentences, decoding_method='viterbi')\n",
        "print(f\"F1 Score (Viterbi, training data): {f1_viterbi:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dz3yDKQswQn",
        "outputId": "b69ab344-0136-445a-9f4b-5505c76c33aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating on Training Data ---\n",
            "Greedy decoding accuracy on training data: 0.9480\n",
            "Viterbi decoding accuracy on training data: 0.9678\n",
            "\n",
            "Confusion Matrix (Greedy, training data):\n",
            "[[  7050     59    802    321     65]\n",
            " [   305   2821   1163    262     42]\n",
            " [    25    136 168926    333    158]\n",
            " [  1017    253   2463   6204     88]\n",
            " [    48    103   2834    104   8039]]\n",
            "Confusion Matrix Labels: ['LOC', 'MISC', 'O', 'ORG', 'PER']\n",
            "F1 Score (Greedy, training data): 0.9445\n",
            "\n",
            "Confusion Matrix (Viterbi, training data):\n",
            "[[  7215     38    584    408     52]\n",
            " [    97   3417    784    219     76]\n",
            " [    34    248 168351    561    384]\n",
            " [   376     85   1603   7900     61]\n",
            " [    33     44    762     98  10191]]\n",
            "F1 Score (Viterbi, training data): 0.9670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6P0Qs5JM0x3"
      },
      "source": [
        "### Train and evaluate HMMs.\n",
        "\n",
        "In this section, you will implement the logic for training and evaluating the HMMs:\n",
        "- Train the model by calling the functions/classes you implemented above,\n",
        "- Evaluate the trained model on the training and evaluation set by calculating the accuracy of the predicted tags.\n",
        "- Compute the confusion matrix and F1 score of the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFcYYThhPRbh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vRsc-YrRAb2"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McZ9KpeQRlA9"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and load the data from the following links\n",
        "\n",
        "https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "\n",
        "https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "\n",
        "Then load the data using what you implemented"
      ],
      "metadata": {
        "id": "mbeC7vllVJdw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9cec1c1",
        "outputId": "3475358f-edbd-4c26-a5b8-7c936c32c4d3"
      },
      "source": [
        "import requests\n",
        "\n",
        "val_data_url = \"https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\"\n",
        "response = requests.get(val_data_url)\n",
        "response.raise_for_status() # Raise an exception for HTTP errors\n",
        "val_data_lines = response.text.splitlines()\n",
        "\n",
        "print(f\"Successfully downloaded {len(val_data_lines)} lines of validation data.\")\n",
        "print(\"First 5 lines:\")\n",
        "for i in range(min(5, len(val_data_lines))):\n",
        "    print(val_data_lines[i])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded 53015 lines of validation data.\n",
            "First 5 lines:\n",
            "CRICKET NNP I-NP O\n",
            "- : O O\n",
            "LEICESTERSHIRE NNP I-NP ORG\n",
            "TAKE NNP I-NP O\n",
            "OVER IN I-PP O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6973a0a3",
        "outputId": "6b3a919f-ee70-4571-8c1e-f3d7eaac0ff4"
      },
      "source": [
        "processed_val_data = PreprocessData(val_data_lines)\n",
        "\n",
        "print(f\"Number of sentences in validation data: {len(processed_val_data.sentences)}\")\n",
        "print(f\"Number of unique words in validation data: {len(processed_val_data.word2id)}\")\n",
        "print(f\"Number of unique tags in validation data: {len(processed_val_data.tag2id)}\")\n",
        "print(\"First sentence (word, tag) tuples from validation data:\")\n",
        "print(processed_val_data.sentences[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in validation data: 3490\n",
            "Number of unique words in validation data: 8697\n",
            "Number of unique tags in validation data: 5\n",
            "First sentence (word, tag) tuples from validation data:\n",
            "[('CRICKET', 'O'), ('-', 'O'), ('LEICESTERSHIRE', 'ORG'), ('TAKE', 'O'), ('OVER', 'O'), ('AT', 'O'), ('TOP', 'O'), ('AFTER', 'O'), ('INNINGS', 'O'), ('VICTORY', 'O'), ('.', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzNnUiBZS4ME"
      },
      "source": [
        "#### Experiment with an HMM with greedy decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "h0t7W9JMTfLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57839571-44ed-494d-a3b0-5ce570a98a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating on Validation Data for Greedy Decoding---\n",
            "Greedy decoding accuracy on validation data: 0.9169\n",
            "\n",
            "Confusion Matrix (Greedy, validation data):\n",
            "[[ 1505    22   261   173    14]\n",
            " [   58   506   339    98     6]\n",
            " [   46    92 40622   379    25]\n",
            " [  263    51   712  1176    48]\n",
            " [   23    19  1231   217  1200]]\n",
            "Confusion Matrix Labels: ['LOC', 'MISC', 'O', 'ORG', 'PER']\n",
            "F1 Score (Greedy, validation data): 0.9096\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Evaluating on Validation Data for Greedy Decoding---\")\n",
        "\n",
        "# Evaluate with greedy decoding on validation set\n",
        "greedy_val_accuracy = hmm_model.evaluate_accuracy(processed_val_data.sentences, decoding_method='greedy')\n",
        "print(f\"Greedy decoding accuracy on validation data: {greedy_val_accuracy:.4f}\")\n",
        "\n",
        "# Compute Confusion Matrix for greedy decoding on validation data\n",
        "cm_greedy_val, cm_labels_val = hmm_model.compute_confusion_matrix(processed_val_data.sentences, decoding_method='greedy')\n",
        "print(f\"\\nConfusion Matrix (Greedy, validation data):\\n{cm_greedy_val}\")\n",
        "print(f\"Confusion Matrix Labels: {cm_labels_val}\")\n",
        "\n",
        "# Compute F1 Score for greedy decoding on validation data\n",
        "f1_greedy_val = hmm_model.compute_f1_score(processed_val_data.sentences, decoding_method='greedy')\n",
        "print(f\"F1 Score (Greedy, validation data): {f1_greedy_val:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfHIDQNKQu4"
      },
      "source": [
        "**(a) Which pair of tags does the model have most difficulty separating according to the confusion matrix of the validation set?**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP4iMShYX3Xp"
      },
      "source": [
        "#### Experiment with an HMM with viterbi decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_zBEh-TpX8KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b202c16-b3b8-4efa-d92b-993439b97926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating on Validation Data for Viterbi Decoding---\n",
            "\n",
            "Viterbi decoding accuracy on validation data: 0.9290\n",
            "\n",
            "Confusion Matrix (Viterbi, validation data):\n",
            "[[ 1509    21   217   194    34]\n",
            " [   19   652   239    77    20]\n",
            " [   52   140 40308   485   179]\n",
            " [  145    30   506  1529    40]\n",
            " [   20    12   827   228  1603]]\n",
            "F1 Score (Viterbi, validation data): 0.9269\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Evaluating on Validation Data for Viterbi Decoding---\")\n",
        "\n",
        "# Evaluate with Viterbi decoding on validation set\n",
        "viterbi_val_accuracy = hmm_model.evaluate_accuracy(processed_val_data.sentences, decoding_method='viterbi')\n",
        "print(f\"\\nViterbi decoding accuracy on validation data: {viterbi_val_accuracy:.4f}\")\n",
        "\n",
        "# Compute Confusion Matrix for Viterbi decoding on validation data\n",
        "cm_viterbi_val, _ = hmm_model.compute_confusion_matrix(processed_val_data.sentences, decoding_method='viterbi')\n",
        "print(f\"\\nConfusion Matrix (Viterbi, validation data):\\n{cm_viterbi_val}\")\n",
        "\n",
        "# Compute F1 Score for Viterbi decoding on validation data\n",
        "f1_viterbi_val = hmm_model.compute_f1_score(processed_val_data.sentences, decoding_method='viterbi')\n",
        "print(f\"F1 Score (Viterbi, validation data): {f1_viterbi_val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c94f910e",
        "outputId": "22eb1bd2-1814-4288-fcf1-cfebfff36a33"
      },
      "source": [
        "all_true_tags_val = []\n",
        "all_predicted_tags_val_greedy = []\n",
        "all_predicted_tags_val_viterbi = []\n",
        "\n",
        "for sentence_data in processed_val_data.sentences:\n",
        "    words = [item[0] for item in sentence_data]\n",
        "    true_tags = [item[1] for item in sentence_data]\n",
        "\n",
        "    predicted_tags_greedy = hmm_model.greedy_decode(words)\n",
        "    predicted_tags_viterbi = hmm_model.viterbi_decode(words)\n",
        "\n",
        "    all_true_tags_val.extend(true_tags)\n",
        "    all_predicted_tags_val_greedy.extend(predicted_tags_greedy)\n",
        "    all_predicted_tags_val_viterbi.extend(predicted_tags_viterbi)\n",
        "\n",
        "print(f\"Total true tags collected: {len(all_true_tags_val)}\")\n",
        "print(f\"Total greedy predicted tags collected: {len(all_predicted_tags_val_greedy)}\")\n",
        "print(f\"Total Viterbi predicted tags collected: {len(all_predicted_tags_val_viterbi)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total true tags collected: 49086\n",
            "Total greedy predicted tags collected: 49086\n",
            "Total Viterbi predicted tags collected: 49086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a88a432",
        "outputId": "2217cf1c-f85d-42d0-afee-2d17fda0a7f7"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "tags = cm_labels_val # Use the sorted unique tags\n",
        "num_tags = len(tags)\n",
        "\n",
        "def calculate_pairwise_discrimination_score(confusion_matrix_obj, tags_list, tag1, tag2):\n",
        "    \"\"\"\n",
        "    Calculates the 'percentage of correct predictions' for a pair of tags\n",
        "    based on their entries in the overall confusion matrix.\n",
        "    Metric: (TP1 + TP2) / (TP1 + TP2 + FN1_as_T2 + FN2_as_T1)\n",
        "    where:\n",
        "    - TP1: True tag1 predicted as tag1\n",
        "    - TP2: True tag2 predicted as tag2\n",
        "    - FN1_as_T2: True tag1 predicted as tag2\n",
        "    - FN2_as_T1: True tag2 predicted as tag1\n",
        "    \"\"\"\n",
        "    idx1 = tags_list.index(tag1)\n",
        "    idx2 = tags_list.index(tag2)\n",
        "\n",
        "    # Number of times tag1 was correctly predicted as tag1\n",
        "    tp1 = confusion_matrix_obj[idx1, idx1]\n",
        "    # Number of times tag2 was correctly predicted as tag2\n",
        "    tp2 = confusion_matrix_obj[idx2, idx2]\n",
        "\n",
        "    # Number of times tag1 was actually tag1 but predicted as tag2\n",
        "    fn1_as_t2 = confusion_matrix_obj[idx1, idx2]\n",
        "    # Number of times tag2 was actually tag2 but predicted as tag1\n",
        "    fn2_as_t1 = confusion_matrix_obj[idx2, idx1]\n",
        "\n",
        "    numerator = tp1 + tp2\n",
        "    denominator = numerator + fn1_as_t2 + fn2_as_t1\n",
        "\n",
        "    if denominator == 0:\n",
        "        return np.nan\n",
        "    return numerator / denominator\n",
        "\n",
        "print(\"\\n--- Pairwise Discrimination Scores (Greedy Decoding, Validation Data) ---\")\n",
        "pairwise_discrimination_scores_greedy = np.full((num_tags, num_tags), np.nan)\n",
        "for i in range(num_tags):\n",
        "    for j in range(i + 1, num_tags):\n",
        "        tag1 = tags[i]\n",
        "        tag2 = tags[j]\n",
        "        score = calculate_pairwise_discrimination_score(cm_greedy_val, tags, tag1, tag2)\n",
        "        pairwise_discrimination_scores_greedy[i, j] = score\n",
        "\n",
        "# Print the upper triangular matrix\n",
        "print(\"Greedy Decoding Pairwise Discrimination Scores:\")\n",
        "print(\" \" * 8 + \" \".join([f\"{tag:>7}\" for tag in tags]))\n",
        "for i in range(num_tags):\n",
        "    print(f\"{tags[i]:<7}\", end=\" \")\n",
        "    for j in range(num_tags):\n",
        "        if j > i:\n",
        "            score_val = pairwise_discrimination_scores_greedy[i, j]\n",
        "            if np.isnan(score_val):\n",
        "                print(f\"{'':>7}\", end=\" \") # Empty space for NaN\n",
        "            else:\n",
        "                print(f\"{score_val:7.4f}\", end=\" \")\n",
        "        else:\n",
        "            print(f\"{'':>7}\", end=\" \")\n",
        "    print()\n",
        "\n",
        "\n",
        "print(\"\\n--- Pairwise Discrimination Scores (Viterbi Decoding, Validation Data) ---\")\n",
        "pairwise_discrimination_scores_viterbi = np.full((num_tags, num_tags), np.nan)\n",
        "for i in range(num_tags):\n",
        "    for j in range(i + 1, num_tags):\n",
        "        tag1 = tags[i]\n",
        "        tag2 = tags[j]\n",
        "        score = calculate_pairwise_discrimination_score(cm_viterbi_val, tags, tag1, tag2)\n",
        "        pairwise_discrimination_scores_viterbi[i, j] = score\n",
        "\n",
        "# Print the upper triangular matrix\n",
        "print(\"Viterbi Decoding Pairwise Discrimination Scores:\")\n",
        "print(\" \" * 8 + \" \".join([f\"{tag:>7}\" for tag in tags]))\n",
        "for i in range(num_tags):\n",
        "    print(f\"{tags[i]:<7}\", end=\" \")\n",
        "    for j in range(num_tags):\n",
        "        if j > i:\n",
        "            score_val = pairwise_discrimination_scores_viterbi[i, j]\n",
        "            if np.isnan(score_val):\n",
        "                print(f\"{'':>7}\", end=\" \") # Empty space for NaN\n",
        "            else:\n",
        "                print(f\"{score_val:7.4f}\", end=\" \")\n",
        "        else:\n",
        "            print(f\"{'':>7}\", end=\" \")\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pairwise Discrimination Scores (Greedy Decoding, Validation Data) ---\n",
            "Greedy Decoding Pairwise Discrimination Scores:\n",
            "            LOC    MISC       O     ORG     PER\n",
            "LOC              0.9617  0.9928  0.8601  0.9865 \n",
            "MISC                     0.9896  0.9186  0.9856 \n",
            "O                                0.9746  0.9708 \n",
            "ORG                                      0.8997 \n",
            "PER                                             \n",
            "\n",
            "--- Pairwise Discrimination Scores (Viterbi Decoding, Validation Data) ---\n",
            "Viterbi Decoding Pairwise Discrimination Scores:\n",
            "            LOC    MISC       O     ORG     PER\n",
            "LOC              0.9818  0.9936  0.8996  0.9829 \n",
            "MISC                     0.9908  0.9532  0.9860 \n",
            "O                                0.9769  0.9766 \n",
            "ORG                                      0.9212 \n",
            "PER                                             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJjLjPCvK6Jr"
      },
      "source": [
        "**(b) What major differences do you observe compared to the matrix in (a)**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Prompts\n",
        "\n",
        "If you used an AI tool to complete any part of this assignment, please paste all prompts you used to produce your final code/responses in the box below and answer the following reflection question."
      ],
      "metadata": {
        "id": "l_g9Yo6JU-9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts Used:\n",
        "*   for each line in the document, the first element is the word (except for -DOCSTART-, which should be ignored), and only the 4th element represents the entity tag. So, for the list of sentences, end the sentence when you encounter an empty line, and make the list contain tuples of the word (1st position) and the tag (4th position).\n",
        "*   Create a class named HMM_Train, where the constructor takes in the list of sentences to be generated from data preprocessing, and the word to id and tag to id dictionaries, and goes through the list, while adding the counts of transitions from one tag to the next (consider for the beginning of the sentence that the tag is a void tag, whose position in the tag2id is 0) to a numpy matrix, and the counts of emission from a tag to a word to another numpy matrix\n",
        "*   for the validation testing, based on the confusion matrix, also calculate the f1 score for all the possible pairs of tags and save them in a strictly upper triangular matrix to be printed. do this for both greedy and Viterbi decoding\n",
        "\n"
      ],
      "metadata": {
        "id": "smbo1v_FVCPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflection: What parts of the AI generated output required modification or improvement? Describe the feedback you gave the tool to produce your final output or any changes you had to make on your own.**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)"
      ],
      "metadata": {
        "id": "R31wWAgxVDMd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}